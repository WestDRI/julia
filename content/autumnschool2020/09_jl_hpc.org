#+title: HPC with Julia: a short introduction
#+description: Hands-on
#+colordes: #8a2000
#+slug: 09_jl_hpc
#+weight: 9

* Jupyter vs SSH

** Limitations of Jupyter

Jupyter is a fantastic tool. It has a major downside however: when you launch a Jupyter session, you are running a job on a compute node. If you want to play for 8 hours in Jupyter, you are requesting an 8 hour job. Now, most of the time you spend on Jupyter is spent typing, running bits and pieces of code, or doing nothing at all. If you ask for GPUs, many CPUs, and lots of RAM, all of it will remain idle most of the time. This is a suboptimal use of resources.

In addition, if you ask for lots of resources for a long time, you will have to wait for a while before they get allocated to you.

Lastly, you will go through your allocations quickly.

** A good approach

A pretty good strategy is to develop and test your code with small samples, few iterations, etc. in an interactive job (from an SSH session in the cluster with ~salloc~), on your own computer (if appropriate), or in Jupyter, *then*, launch an ~sbatch~ job from an SSH session in the cluster to run the full code. This ensures that heavy duty resources such as GPU(s) are only allocated to you when you are actually needing and using them.

This is exactly what we are doing during this workshop.

* Accessing Julia in an SSH session

** Log in the training cluster

First, [[https://westgrid-julia.netlify.app/autumnschool2020/01_jl_intro/#headline-3][SSH into our training cluster UU]]. This takes you to the login node of the cluster.

** Load necessary modules

This is done with the [[https://github.com/TACC/Lmod][Lmod]] tool through the [[https://docs.computecanada.ca/wiki/Utiliser_des_modules/en][module command]] (you can find the full documentation [[https://lmod.readthedocs.io/en/latest/010_user.html][here]]).

Below are some key [[https://lmod.readthedocs.io/en/latest/010_user.html][Lmod commands]]:

#+BEGIN_src sh
# Get help on the module command
$ module help

# List modules that are already loaded
$ module list

# See which modules are available for Julia
$ module spider julia

# See how to load the latest version (julia/1.5.2)
$ module spider julia/1.5.2

# Load julia/1.5.2 with the required StdEnv/2020 module first
# (the order is important)
$ module load StdEnv/2020 julia/1.5.2

# You can see that we now have Julia loaded
$ module list
#+END_src

# ** Install Julia packages

# {{<challenge>}}
# Install the package {{<b>}}BenchmarkTools{{</b>}}
# {{</challenge>}}

# - After loading the proper modules, launch Julia:

# #+BEGIN_src sh
# $ julia
# #+END_src

# - Within Julia, type ~]~

# #+BEGIN_export html
# (you will see your prompt change from <span style="font-family: 'Source Code Pro', 'Lucida Console', monospace; font-size: 1.4rem; padding: 0.2rem; border-radius: 5%; background-color: #f0f3f3; color: #339933"><b>julia></b></span> to <span style="font-family: 'Source Code Pro', 'Lucida Console', monospace; font-size: 1.4rem; padding: 0.2rem; border-radius: 5%; background-color: #f0f3f3; color: #2e5cb8"><b>pkg></b></span>) <br>
# #+END_export

# - then run:

# #+BEGIN_src julia
# add BenchmarkTools
# #+END_src

* Running jobs on the cluster

You should never run computing tasks on the login node. You need to submit a job to Slurm (the job scheduler used by the Compute Canada clusters) to access the computing nodes of the cluster.

** Interactive jobs

To launch an interactive session on a compute node, use the [[https://slurm.schedmd.com/salloc.html][salloc command]].

#+BEGIN_ex
Example:
#+END_ex

#+BEGIN_src sh
$ salloc -t 10 -c 8 --mem=2G
#+END_src

This will send you on a compute node for 10 minutes. On that node, you will have access to 8 CPU and 2G of RAM. This is a place where you can launch ~julia~ and run some computations. At the end of the 10 min, or if you cancel the job with {{<b>}}Ctrl-D{{</b>}}, you will get back to the login node.

Interactive jobs have the same drawback as Jupyter. To limit your resource allocations to what you really need, you want to submit jobs to Slurm with [[https://slurm.schedmd.com/sbatch.html][sbatch]].

** Job scripts

To submit a batch job to Slurm, first, you need a Julia script to run.

*** Write a Julia script

Create a directory for your project in ~~/~ and ~cd~ into it:

#+BEGIN_src sh
$ mkdir ~/julia_project
$ cd ~/julia_project
#+END_src

Write a Julia script with the text editor of your choice:

#+BEGIN_src sh
$ nano my_julia_script.jl
#+END_src

*** Write an sbatch script

Then you need to write a shell script for {{<b>}}sbatch{{</b>}}:

#+BEGIN_src sh
$ nano script.sh
#+END_src

The script may look something like this:

#+BEGIN_src sh
#!/bin/bash
#SBATCH --job-name=<name>			# job name
#SBATCH --time=<time>				# max walltime
#SBATCH --nodes=<N>			        # number of nodes
#SBATCH --cpus-per-task=<n>         # number of cores on each node
#SBATCH --mem=<mem>					# max memory (default unit is megabytes)
#SBATCH --output=%j.out				# file name for the output
#SBATCH --error=%j.err				# file name for errors

julia my_julia_script.jl
#+END_src

#+BEGIN_note
Notes:
- ~--time~ accepts these formats: "min", "min:s", "h:min:s", "d-h", "d-h:min" & "d-h:min:s"
- ~%j~ gets replaced with the job number
#+END_note


To submit a job to the cluster:

#+BEGIN_src sh
$ cd /dir/containing/script
$ sbatch script.sh
#+END_src

And we can check its status with:

#+BEGIN_src sh
$ sq
#+END_src

#+BEGIN_note
{{<b>}}PD{{</b>}} = pending\\
{{<b>}}R{{</b>}} = running\\
{{<b>}}CG{{</b>}} = completing (Slurm is doing the closing processes) \\
No information = your job has finished running
#+END_note

You can cancel it with:

#+BEGIN_src sh
$ scancel <jobid>
#+END_src

Once your job has finished running, you can display efficiency measures with:

#+BEGIN_src sh
$ seff <jobid>
#+END_src

* Parallel computing

The whole point of running your Julia script on the cluster is to take advantage of its large computing power to improve the time required for it to run. [[https://docs.julialang.org/en/v1/manual/performance-tips/][There are hardware-independent techniques to optimize your Julia code]]. After that, the key to improve performance is code parallelization through shared memory, distributed memory, and the use of GPUs.

** Shared memory (aka multi-threading)

*** Launching Julia on multiple threads

Starting with Julia 1.5, you can launch Julia on ~n~ threads with:

#+BEGIN_src sh
$ julia -t n
#+END_src

#+BEGIN_ex
For example, to launch Julia on 4 threads, you can run:
#+END_ex

#+BEGIN_src sh
$ julia -t 4
#+END_src

For earlier versions, you need to set the {{<b>}}JULIA_NUM_THREADS{{</b>}} environment variable:

#+BEGIN_src sh
$ export JULIA_NUM_THREADS=n
$ julia
#+END_src

Or you can launch Julia with:

#+BEGIN_src sh
$ JULIA_NUM_THREADS=n julia
#+END_src

#+BEGIN_ex
For example, to launch Julia on 4 threads, you can run:
#+END_ex

#+BEGIN_src sh
$ JULIA_NUM_THREADS=4 julia
#+END_src

*** Using multiple threads

Multi-threading is supported by the {{<b>}}Base.Threads{{</b>}} module.

~Threads.nthreads()~ outputs the number of threads Julia is using and ~Threads.threadid()~ outputs the ID of the current thread.

The ~Threads.@threads~ macro allows to run for loops on multiple threads extremely easily.

#+BEGIN_ex
Example:
#+END_ex

#+BEGIN_src julia
Threads.@threads for i = 1:10
    println("i = $i on thread $(Threads.threadid())")
end
#+END_src

The ~Threads.@spawn~ macro allows multi-threading outside the context of loops. This feature is currently experimental and little documented, but an example is given in [[https://julialang.org/blog/2019/07/multithreading/][this blog post]].

** Distributed computing

*** Launching several Julia processes

Julia supports distributed computing thanks to the module {{<b>}}Distributed{{</b>}}.

There are two ways to launch several Julia processes (called "workers"):

**** Launch Julia on n workers

Julia can be started with the ~-p~ flag followed by the number of workers by running:

#+BEGIN_src sh
$ julia -p n
#+END_src

This launches {{<b>}}n{{</b>}} workers, available for parallel computations, in addition to the process running the interactive prompt, so there are {{<b>}}n + 1{{</b>}} Julia processes in total.

#+BEGIN_ex
Example to start 4 worker processes:
#+END_ex

#+BEGIN_src sh
$ julia -p 4
#+END_src

Launching Julia with the ~-p~ flag automatically loads the {{<b>}}Distributed{{</b>}} module.

**** Start workers from within a Julia session

Alternatively, workers can be started from within a Julia session. In this case, you need to load the module {{<b>}}Distributed{{</b>}} explicitly:

#+BEGIN_src julia
using Distributed
#+END_src

To launch {{<b>}}n{{</b>}} workers:

#+BEGIN_src julia
addprocs(n)
#+END_src

#+BEGIN_ex
Example to add 4 worker processes to a running Julia session:
#+END_ex

#+BEGIN_src julia
addprocs(4)
#+END_src

*** Managing workers

In Julia, you can see how many workers are running with:

#+BEGIN_src julia
nworkers()
#+END_src

The total number of processes ({{<b>}}n + 1{{</b>}}) can be returned with:

#+BEGIN_src julia
nprocs()
#+END_src

You can list all the worker process identifiers with:

#+BEGIN_src julia
workers()
#+END_src

#+BEGIN_ex
The process running the Julia prompt has id {{<b>}}1{{</b>}}.
#+END_ex

To kill a worker:

#+BEGIN_src julia
rmprocs(<pid>)
#+END_src

#+BEGIN_note
where ~<pid>~ is the process identifier of the worker you want to kill (you can kill several workers by providing a list of pids).
#+END_note

*** Using workers

There are a number of convenient macros:

**** @everywhere

*The following expression gets executed on all processes.*

For instance, if your parallel code requires a module or an external package to run, you need to load that module or package with ~@everywhere~:

#+BEGIN_src julia
@everywhere using DataFrames
#+END_src

If the parallel code requires a script to run:

#+BEGIN_src julia
@everywhere include("script.jl")
#+END_src

If it requires a function that you are defining, you need to define it on all the workers:

#+BEGIN_src julia
@everywhere function <name>(<arguments>)
    <body>
end
#+END_src

**** @spawnat

*Assigns a task to a particular worker.*

The first argument indicates the process id, the second argument is the expression that should be evaluated:

#+BEGIN_src julia
@spawnat <pid> <expression>
#+END_src

~@spawnat~ returns a {{<b>}}Future{{</b>}}: the placeholder for a computation of unknown status and time. The function ~fetch~ waits for a {{<b>}}Future{{</b>}} to complete and returns the result of the computation.

#+BEGIN_ex
Example:
#+END_ex

The function ~myid~ gives the id of the current process. As I mentioned earlier, the process running the interactive Julia prompt has the pid {{<b>}}1{{</b>}}. So ~myid()~ normally returns ~1~.

But we can "spawn" ~myid~ on one of the worker, for instance the first worker (so pid {{<b>}}2{{</b>}}):

#+BEGIN_src julia
@spawnat 2 myid()
#+END_src

This returns a {{<b>}}Future{{</b>}}, but if we pass it through ~fetch~, we get the result of ~myid~ ran on the worker with pid {{<b>}}2{{</b>}}:

#+BEGIN_src julia
fetch(@spawnat 2 myid())
#+END_src

If you want tasks to be assigned to any worker automatically, you can pass the symbol ~:any~ to ~@spawnat~ instead of the worker id:

#+BEGIN_src julia
@spawnat :any myid()
#+END_src

And to get the result:

#+BEGIN_src julia
fetch(@spawnat :any myid())
#+END_src

If you run this multiple times, you will see that ~myid~ is run on any of your available workers. This will however never return ~1~, /except/ when you only have one running Julia process (in that case, the process running the prompt is considered a worker).

*** Data too large to fit in the memory of a single node

In the case of extremely large data which cannot fit in memory on a single node, the [[https://github.com/JuliaParallel/DistributedArrays.jl][DistributedArrays]] package allows to distribute large arrays across multiple nodes.

** GPUs

Julia has [[https://github.com/JuliaGPU][GPU support through a number of packages]]. We will offer workshops and webinars on running Julia on GPUs in 2021.

* Comments & questions
