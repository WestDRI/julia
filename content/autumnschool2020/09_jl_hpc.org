#+title: HPC with Julia: a short introduction
#+description: Hands-on
#+colordes: #8a2000
#+slug: 09_jl_hpc
#+weight: 9

* Logging to the cluster
* Jupyter vs SSH

Open a terminal emulator.
** Limitations of Jupyter

/Windows users, launch [[https://mobaxterm.mobatek.net/][MobaXTerm]]./ \\
/MacOS users, launch Terminal./ \\
/Linux users, launch xterm or the terminal emulator of your choice./
Jupyter is a fantastic tool. It has a major downside however: when you launch a Jupyter session, you are running a job on a compute node. If you want to play for 8 hours in Jupyter, you are requesting an 8 hour job. Now, most of the time you spend on Jupyter is spent typing, running bits and pieces of code, or doing nothing at all. If you ask for GPUs, many CPUs, and lots of RAM, all of it will remain idle most of the time. This is a suboptimal use of resources.

#+BEGIN_src sh
$ ssh userxxx@cassiopeia.c3.ca
In addition, if you ask for lots of resources for a long time, you will have to wait for a while before they get allocated to you.

# enter password
#+END_src
You will also go through your allocations quickly.

You are now in our training cluster.
Lastly, your notebook is running on a single node. It is thus not suitable to run distributed memory parallel tasks.

* Loading the Julia module
** A good approach

This is done with the [[https://github.com/TACC/Lmod][Lmod]] tool through the [[https://docs.computecanada.ca/wiki/Utiliser_des_modules/en][module]] command. You can find the full documentation [[https://lmod.readthedocs.io/en/latest/010_user.html][here]] and below are the subcommands you will need:
A pretty good strategy is to develop and test your code with small samples, few iterations, etc. in an interactive job (from an SSH session in the cluster with ~salloc~), on your own computer (if appropriate), or in Jupyter, *then*, launch an ~sbatch~ job from an SSH session in the cluster to run the full code. This ensures that heavy duty resources such as GPU(s) are only allocated to you when you are actually needing and using them.

This is exactly what we are doing during this workshop.

#+BEGIN_src sh
# Get help on the module command
$ module help
$ module --help
$ module -h

# List modules that are already loaded
$ module list

# See which modules are available for Julia
$ module spider julia

# See how to load the latest version (julia 1.4.1)
$ module spider julia/1.4.1

# Load julia 1.4.1 with the required gcc module first
# (the order is important)
$ module load gcc/7.3.0 julia/1.4.1

# You can see that we now have Julia loaded
$ module list
#+END_src

* Copying files to the cluster

If you need to copy files to the cluster, you can use {{<c>}}scp{{</c>}}.

**** From your computer

If you are in a local shell, run:

#+BEGIN_src sh
[local]$ scp /local/path/file  userxxx@cassiopeia.c3.ca:path/cluster
#+END_src

**** From the cluster

If you are in a remote shell (through ssh), run:

#+BEGIN_src sh
[cluster]$ scp userxxx@cassiopeia.c3.ca:cluster/path/file  /local/path
#+END_src

* Job scripts

To submit a job to Slurm (the job scheduler used by the Compute Canada clusters), you need to write an {{<b>}}sbatch{{</b>}} script:

#+BEGIN_src sh
#!/bin/bash
#SBATCH --job-name=<name>			# job name
#SBATCH --time=<time>				# max walltime
#SBATCH --cpus-per-task=<n>         # number of cores
#SBATCH --mem=<mem>					# max memory (default unit is megabytes)
#SBATCH --output=<file%j.out>		# file name for the output
#SBATCH --error=<file%j.err>		# file name for errors
# %j gets replaced with the job number

<body>
#+END_src

To submit a job to the cluster:

#+BEGIN_src sh
$ cd /dir/containing/script
$ sbatch script.sh
#+END_src

And we can check its status with:

#+BEGIN_src sh
$ sq
#+END_src

{{<b>}}PD{{</b>}} stands for pending and {{<b>}}R{{</b>}} for running.

* Comments & questions
