#+title: HPC with Julia: a short introduction
#+description: Hands-on
#+colordes: #8a2000
#+slug: 09_jl_hpc
#+weight: 9

* Jupyter vs SSH

** Limitations of Jupyter

Jupyter is a fantastic tool. It has a major downside however: when you launch a Jupyter session, you are running a job on a compute node. If you want to play for 8 hours in Jupyter, you are requesting an 8 hour job. Now, most of the time you spend on Jupyter is spent typing, running bits and pieces of code, or doing nothing at all. If you ask for GPUs, many CPUs, and lots of RAM, all of it will remain idle most of the time. This is a suboptimal use of resources.

In addition, if you ask for lots of resources for a long time, you will have to wait for a while before they get allocated to you.

Lastly, you will go through your allocations quickly.

** A good approach

A pretty good strategy is to develop and test your code with small samples, few iterations, etc. in an interactive job (from an SSH session in the cluster with ~salloc~), on your own computer (if appropriate), or in Jupyter, *then*, launch an ~sbatch~ job from an SSH session in the cluster to run the full code. This ensures that heavy duty resources such as GPU(s) are only allocated to you when you are actually needing and using them.

This is exactly what we are doing during this workshop.

* Accessing Julia in an SSH session

** Log in the training cluster

First, [[https://westgrid-julia.netlify.app/autumnschool2020/01_jl_intro/#headline-3][SSH into our training cluster UU]]. This takes you to the login node of the cluster.

** Load necessary modules

This is done with the [[https://github.com/TACC/Lmod][Lmod]] tool through the [[https://docs.computecanada.ca/wiki/Utiliser_des_modules/en][module command]] (you can find the full documentation [[https://lmod.readthedocs.io/en/latest/010_user.html][here]]).

Below are some key [[https://lmod.readthedocs.io/en/latest/010_user.html][Lmod commands]]:

#+BEGIN_src sh
# Get help on the module command
$ module help

# List modules that are already loaded
$ module list

# See which modules are available for Julia
$ module spider julia

# See how to load the latest version (julia/1.5.2)
$ module spider julia/1.5.2

# Load julia/1.5.2 with the required StdEnv/2020 module first
# (the order is important)
$ module load StdEnv/2020 julia/1.5.2

# You can see that we now have Julia loaded
$ module list
#+END_src

* Running jobs on the cluster

You should never run computing tasks on the login node. You need to submit a job to Slurm (the job scheduler used by the Compute Canada clusters) to access the computing nodes of the cluster.

** Interactive jobs

To launch an interactive session on a compute node, use the [[https://slurm.schedmd.com/salloc.html][salloc command]].

#+BEGIN_ex
Example:
#+END_ex

#+BEGIN_src sh
$ salloc -t 10 -c 8 --mem=2G
#+END_src

This will send you on a compute node for 10 minutes. On that node, you will have access to 8 CPU and 2G of RAM. This is a place where you can launch ~julia~ and run some computations. At the end of the 10 min, or if you cancel the job with {{<b>}}Ctrl-D{{</b>}}, you will get back to the login node.

Interactive jobs have the same drawback as Jupyter. To limit your resource allocations to what you really need, you want to submit jobs to Slurm with [[https://slurm.schedmd.com/sbatch.html][sbatch]].

** Job scripts

To submit a batch job to Slurm, first, you need a Julia script to run.

*** Write a Julia script

Create a directory for your project in ~~/~ and ~cd~ into it:

#+BEGIN_src sh
$ mkdir ~/julia_project
$ cd ~/julia_project
#+END_src

Write a Julia script with the text editor of your choice:

#+BEGIN_src sh
$ nano my_julia_script.jl
#+END_src

*** Write an sbatch script

Then you need to write a shell script for {{<b>}}sbatch{{</b>}}:

#+BEGIN_src sh
$ nano script.sh
#+END_src

The script may look something like this:

#+BEGIN_src sh
#!/bin/bash
#SBATCH --job-name=<name>			# job name
#SBATCH --time=<time>				# max walltime
#SBATCH --nodes=<N>			        # number of nodes
#SBATCH --cpus-per-task=<n>         # number of cores on each node
#SBATCH --mem=<mem>					# max memory (default unit is megabytes)
#SBATCH --output=<file%j.out>		# file name for the output
#SBATCH --error=<file%j.err>		# file name for errors
# %j gets replaced with the job number

julia my_julia_script.jl
#+END_src

#+BEGIN_note
Notes:
- ~--time~ accepts these formats: "min", "min:s", "h:min:s", "d-h", "d-h:min" & "d-h:min:s"
- ~%x~ will get replaced by the script name & ~%j~ by the job number
#+END_note


To submit a job to the cluster:

#+BEGIN_src sh
$ cd /dir/containing/script
$ sbatch script.sh
#+END_src

And we can check its status with:

#+BEGIN_src sh
$ sq
#+END_src

#+BEGIN_note
{{<b>}}PD{{</b>}} = pending\\
{{<b>}}R{{</b>}} = running\\
{{<b>}}CG{{</b>}} = completing (Slurm is doing the closing processes) \\
No information = your job has finished running
#+END_note

You can cancel it with:

#+BEGIN_src sh
$ scancel <jobid>
#+END_src

Once your job has finished running, you can display efficiency measures with:

#+BEGIN_src sh
$ seff <jobid>
#+END_src

* Comments & questions
